{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data and Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1521787,), (421665,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_train = np.load('data/key_train.npy' )\n",
    "key_test = np.load('data/key_test.npy' )\n",
    "key_train.shape, key_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1521787, 243), (421665, 243))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_col_names = np.load('data/X_col_names.npy' )\n",
    "X_train = np.load('data/X_train.npy' )\n",
    "X_test = np.load('data/X_test.npy' )\n",
    "y_train = np.load('data/y_train.npy' )\n",
    "n_train = len(y_train)\n",
    "ind_to_be_del  = np.load('data/ind_to_be_del.npy' )\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1521787,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weight_train = np.load('data/sample_weight.npy' )\n",
    "sample_weight_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1521787, 248), (421665, 248))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional \n",
    "X_pca = np.load(\"data/X_pca5.npy\")\n",
    "X = np.concatenate((np.concatenate((X_train,X_test),0), X_pca), 1)\n",
    "X_train, X_test = X[:n_train, :], X[n_train:, :]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1521787, 328), (421665, 328))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional \n",
    "X_train = np.concatenate((X_train, np.load(\"data_201910_1/X_train_CONTSP.npy\")),1)\n",
    "X_test = np.concatenate((X_test, np.load(\"data_201910_1/X_test_CONTSP.npy\")),1)\n",
    "X = np.concatenate((X_train,X_test), 0)\n",
    "X_train, X_test = X[:n_train, :], X[n_train:, :]\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete y==1 nerghbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y==1的比率:  0.013375722095142093\n"
     ]
    }
   ],
   "source": [
    "print( \"y==1的比率: \", (y_train>0).sum()/len(y_train) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1488766, 328), (1488766,), (1488766,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del some samples\n",
    "X_train = X_train[~ind_to_be_del]\n",
    "key_train = key_train[~ind_to_be_del]\n",
    "y_train = y_train[~ind_to_be_del]\n",
    "X_train.shape, key_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y==1的比率:  0.01367239713964451\n"
     ]
    }
   ],
   "source": [
    "print( \"y==1的比率: \", (y_train>0).sum()/len(y_train) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down/Up Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1468411, 20355)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_y0, X_train_y1 = X_train[y_train==0], X_train[y_train==1]\n",
    "X_train_y0.shape[0], X_train_y1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(X, n=1):\n",
    "    assert type(X) == np.ndarray and len(X.shape) >= 2\n",
    "    rand_idx = np.random.randint(0, X.shape[0], n)\n",
    "    return X[rand_idx], rand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "734205"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sample = int(X_train_y0.shape[0]/2) #int(np.mean((n_y0, n_y1)))\n",
    "n_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(734205, 40710)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_y0, idx_train_y0 = sampling(X_train_y0, n=n_sample) # downsampling\n",
    "X_train_y1, idx_train_y1 = sampling(X_train_y1, n=X_train_y1.shape[0]*2) # upsampling\n",
    "X_train_y0.shape[0], X_train_y1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_y0, X_train_y1),0)\n",
    "y_train = np.concatenate((np.zeros(X_train_y0.shape[0]), np.ones(X_train_y1.shape[0])),0)\n",
    "idx_train = np.concatenate((idx_train_y0, idx_train_y1),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((774915,), (774915,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_train = key_train[idx_train]\n",
    "sample_weight_train = sample_weight_train[idx_train]\n",
    "key_train.shape, sample_weight_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((774915, 328), (774915,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((542440, 328), (542440,), (232475, 328), (232475,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_, X_val, y_train_, y_val, \\\n",
    "key_train_, key_val, sample_weight_train_, sample_weight_val =  \\\n",
    "train_test_split(X_train, y_train, key_train, sample_weight_train, \n",
    "                 test_size=0.3, random_state=42)\n",
    "X_train_.shape, y_train_.shape, X_val.shape, y_val.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from  datetime import datetime\n",
    "import pickle\n",
    "\n",
    "def score_n_save_out(model, X, y,  \n",
    "                     model_name = \"\",\n",
    "                     postfix=\"0\", \n",
    "                     print_score=True,\n",
    "                     save_model=True, \n",
    "                     save_csv=False):\n",
    "    \n",
    "    y_pred = model.predict(X)\n",
    "    y_pred = (y_pred>0.5).astype(int).flatten()\n",
    "    y = y.astype(int).flatten()\n",
    "    print( \"X_val y_pred==1 ratio : {:.2f}%\".format( sum(y_pred == 1)/len(y_pred)*100 ) )\n",
    "    \n",
    "    if print_score:\n",
    "        f1, p, r = ( f1_score(y, y_pred), precision_score(y, y_pred),  recall_score(y, y_pred))\n",
    "        print(\"model: {}, F1-score: {:.4f}, precision: {:.4f}, recall: {:.4f}\".format(model_name, f1, p, r))\n",
    "        \n",
    "    t_str = datetime.strftime(datetime.now(), \"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if save_model: \n",
    "        pth = f'model/{model_name}_model_{postfix}_{t_str}.pkl'\n",
    "        pickle.dump(model, open(pth, 'wb'))\n",
    "        print(f'save model: {pth}')\n",
    "        \n",
    "    if save_csv:\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = (y_pred>0.5).astype(int).flatten()\n",
    "        print( \"X_test y_pred==1 ratio : {:.2f}%\".format( sum(y_pred == 1)/len(y_pred)*100 ) )\n",
    "        df_op = pd.DataFrame({'txkey':key_test, 'fraud_ind': y_pred})\n",
    "        pth = f'submit/{model_name}_submission_{postfix}_{t_str}.csv'\n",
    "        df_op.to_csv(pth, index=False) \n",
    "        print(f'save scv: {pth}')\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import \\\n",
    "ExtraTreesClassifier, GradientBoostingClassifier, \\\n",
    "RandomForestClassifier, AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = [\n",
    "                 ExtraTreesClassifier(n_estimators=10, class_weight='balanced'),\n",
    "                 ExtraTreesClassifier(n_estimators=100, class_weight='balanced'), \n",
    "                 #ExtraTreesClassifier(n_estimators=1000, class_weight='balanced'),  \n",
    "                ]\n",
    "model_names = [\n",
    "               \"ExtraTrees_n=10\",  \n",
    "               \"ExtraTrees_n=100\", \n",
    "               #\"ExtraTrees_n=1000\",  \n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************\n",
      "X_val y_pred==1 ratio : 5.18%\n",
      "model: ExtraTrees_n=10, F1-score: 0.9477, precision: 0.9561, recall: 0.9395\n",
      "save model: model/ExtraTrees_n=10_model_0_20191005111400.pkl\n",
      "X_test y_pred==1 ratio : 2.25%\n",
      "save scv: submit/ExtraTrees_n=10_submission_0_20191005111400.csv\n",
      "time cost: 34.82 sec\n",
      "****************************************************************\n",
      "X_val y_pred==1 ratio : 5.21%\n",
      "model: ExtraTrees_n=100, F1-score: 0.9497, precision: 0.9555, recall: 0.9441\n",
      "save model: model/ExtraTrees_n=100_model_0_20191005111901.pkl\n",
      "X_test y_pred==1 ratio : 2.39%\n",
      "save scv: submit/ExtraTrees_n=100_submission_0_20191005111901.csv\n",
      "time cost: 311.48 sec\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "models = []\n",
    "for model, model_name in zip(model_classes, model_names):\n",
    "    print('*'*64)\n",
    "    st = time.time()\n",
    "    model.fit(X_train_, y_train_, sample_weight=sample_weight_train_)\n",
    "    models.append(model)\n",
    "    score_n_save_out(model, X_val, y_val, model_name=model_name, save_csv=True, )\n",
    "    print(f\"time cost: {round((time.time()-st),2)} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- submit/ExtraTrees_n=100_submission_0_20191004123336.csv -> 55.5%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Model - Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from  datetime import datetime\n",
    "def score_n_save_out_dl(model, X, y,  \n",
    "                     model_name = \"DNN\",\n",
    "                     postfix=\"0\", \n",
    "                     print_score=True,\n",
    "                     save_model=True, \n",
    "                     save_csv=False):\n",
    "    \n",
    "    y_pred = np.argmax( model.predict(X) , 1)\n",
    "    print(y_pred.shape)\n",
    "\n",
    "    y = y.astype(int).flatten()\n",
    "    print( \"X_val y_pred==1 ratio : {:.2f}%\".format( sum(y_pred == 1)/len(y_pred)*100 ) )\n",
    "    \n",
    "    if print_score:\n",
    "        f1, p, r = ( f1_score(y, y_pred), precision_score(y, y_pred),  recall_score(y, y_pred))\n",
    "        print(\"model: {}, F1-score: {:.4f}, precision: {:.4f}, recall: {:.4f}\".format(model_name, f1, p, r))\n",
    "        \n",
    "    t_str = datetime.strftime(datetime.now(), \"%Y%m%d%H%M%S\")\n",
    "    \n",
    "    if save_model: \n",
    "        pth = f'model/model_{postfix}_{t_str}.h5'\n",
    "        model.save_weights(pth)\n",
    "        print(f'save model: {pth}')\n",
    "        \n",
    "    if save_csv:\n",
    "        y_pred = np.argmax( model.predict(X_test), 1)\n",
    "        print( \"X_test y_pred==1 ratio : {:.2f}%\".format( sum(y_pred == 1)/len(y_pred)*100 ) )\n",
    "        df_op = pd.DataFrame({'txkey':key_test, 'fraud_ind': y_pred})\n",
    "        pth = f'submit/{model_name}_submission_{postfix}_{t_str}.csv'\n",
    "        df_op.to_csv(pth, index=False) \n",
    "        print(f'save scv: {pth}')\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(542440, 2)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train__categorical = to_categorical(y_train_, num_classes=2)\n",
    "y_train__categorical.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model - Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/keras/initializers.py:104: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with distribution=normal is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`normal` is a deprecated alias for `truncated_normal`\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=2048,                    \n",
    "           input_dim=X_train.shape[1],                     \n",
    "           kernel_initializer='he_normal',  \n",
    "           activation=LeakyReLU(0.1))) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=256,                                          \n",
    "           kernel_initializer='he_normal',  \n",
    "           activation=LeakyReLU(0.1))) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=64,                                        \n",
    "           kernel_initializer='he_normal', \n",
    "           activation=LeakyReLU(0.1))) \n",
    "model.add(Dense(units=2,                                        \n",
    "           kernel_initializer='he_normal', \n",
    "           activation='softmax'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.load_weights('model/model_160.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "542440/542440 [==============================] - 5s 9us/step - loss: 4.7338e-05 - weighted_acc: 5.7608e-04\n",
      "Epoch 2/3\n",
      "542440/542440 [==============================] - 3s 6us/step - loss: 3.8876e-05 - weighted_acc: 5.7922e-04\n",
      "Epoch 3/3\n",
      "542440/542440 [==============================] - 4s 7us/step - loss: 3.5222e-05 - weighted_acc: 5.7978e-04\n",
      "(232475,)\n",
      "X_val y_pred==1 ratio : 4.62%\n",
      "model: DNN, F1-score: 0.7517, precision: 0.8044, recall: 0.7054\n",
      "save model: model/model_10_20191005111937.h5\n",
      "X_test y_pred==1 ratio : 2.69%\n",
      "save scv: submit/DNN_submission_10_20191005111937.csv\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',  \n",
    "              optimizer=Adam(lr=3e-3),  \n",
    "              weighted_metrics=['accuracy'])\n",
    "train_history=model.fit(x=X_train_,    \n",
    "                        y=y_train__categorical, \n",
    "                        sample_weight=sample_weight_train_,\n",
    "                        epochs=3,                     \n",
    "                        batch_size=2048, \n",
    "                        class_weight={0:0.7, 1:0.3},\n",
    "                        shuffle=True,\n",
    "                        #callbacks = callbacks_list,\n",
    "                        verbose=1)\n",
    "score_n_save_out_dl(model, X_val, y_val, \n",
    "                 postfix=\"10\", save_csv=True, ) # 上繳分數 0.464427"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "score_n_save_out_dl(model, X_val, y_val, save_csv=True, save_model=False, print_score=False,\n",
    "                 postfix=\"190\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
